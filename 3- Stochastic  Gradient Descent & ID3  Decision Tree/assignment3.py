# -*- coding: utf-8 -*-
"""CS5233_assignment3_llx812.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_vITODO6NBoH1-syTGbtVDedt38aNdyt

# UTSA CS 3793: Assignment-3

**Last Name - First Name - (abc123)**

## Learning Objectives

Implement 2 different machine learning algorithms
*   Stochastic Gradient Descent
*   ID3 Decision Tree

## Description

This assignment is focused on **machine learning**, mainly on the implementation of 2 different algorithms - Stochastic Gradient Descent & ID3 decision tree.
The assignment is divided into two sections, each for one unique ML algorithm.

The base structure and comments are provided on what should be done. You can use some libraries that help support you for the successful completion of the assignment. However, you **CANNOT** use a complete library that contains the implementation of ML algorithms. You can get pieces of code from online, but please cite the source properly.

##Import Libraries

Write all the import statements here. This should be for both algorithm implmentations. As mentioned before, you can not use any premade ML libraries.
"""

# import all required libraries
import numpy as np
import math
import pandas as pd

# Assume that the data files are in the following folder -- THIS WILL BE USED BY THE TA
basePath = "/content/drive/My Drive/Colab Notebooks/Artificial Intelligence/Data/"

"""#Stochastic Gradient Descent

In this section, you will implement the Stochastic Gradient Descent algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that you are given **all binary-valued attributes** and that there are **no missing values** in the train or test data.

##Algorithm

(40 points)

Following are the data files that will be provided to you for the gradient descent algorithm implementation.

*   Training file - 'gd-train.dat'
*   Testing file - 'gd-test.dat'

In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are different example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.

Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments.
"""

# Data file name variables
gd_test_path= "/content/sample_data/id3-test.dat"
gd_train_path ="/content/sample_data/gd-train.dat"

# Read the training and testing data files
def read_data(filename):
    with open(filename, 'r') as file:
        lines = file.readlines()

    # Initialize empty lists to store features and labels
    features = []
    labels = []

    # Skip the header line
    for line in lines[1:]:
        data = line.strip().split('\t')
        try:
            label = int(data[-1])

            features.append(list(map(int, data[:-1])))
            labels.append(label)
        except ValueError:
            continue
            print(f"Warning: Invalid data point with label '{data[-1]}' is skipped.")

    # Convert lists to numpy arrays for easier manipulation

    #print("lab:\n",label)
    #print("feature:\n",features)
    X = np.array(features)
    y = np.array(labels)
    #print("X",X)
    #print("y",y)

    return X, y

# Activation Function - implement Sigmoid
def activation_function(h):
    # given 'h' compute and return 'z' based on the activation function implemented
    return 1 / (1 + math.exp(-h))

# Train the model using the given training dataset and the learning rate
# return the "weights" learnt for the perceptron - include the weight assocaited with bias as the last entry
def train(train_data, learning_rate=0.05):
    # initialize weights to 0
    # go through each training data instance
        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label
        # obtain h(x)
        # call the activation function with 'h' as parameter to obtain 'z'
        # update all weights individually using learning_rate, (y-z), and the corresponding 'x'
    # return the final learnt weights

    # Excluding the last column which contains labels
    num_features = len(train_data[0]) - 1
    num_instances = len(train_data)

    # Initialize weights (including bias weight) to zeros
    weights = [0.0] * (num_features + 1)
    #print("weights:", weights)


    # Initialize weights (including bias weight) to zeros
    weights = [0.0] * (num_features + 1)
    #print("weights:", weights)

    for num_feature in range(num_features):
        for i in range(num_instances):
            # Get one data instance and the corresponding true label
            x = train_data[i][:-1]
            y_true = train_data[i][-1]

            # Compute h(x) = sum(w_i * x_i) + bias
            h = sum(w_i * x_i for w_i, x_i in zip(weights[:-1], x)) + weights[-1]

            # Apply activation function (sigmoid)
            z = activation_function(h)

            # Update the weights based on the error
            error = y_true - z
            weights[:-1] = [w_i + learning_rate * error * x_i for w_i, x_i in zip(weights[:-1], x)]
            weights[-1] += learning_rate * error  # Update bias weight

    return weights

# Test the model (weights learnt) using the given test dataset
# return the accuracy value
def test(test_data, weights, threshold):
    # go through each testing data instance
        # get 'x' as one multi-variate data instance and 'y' as the ground truth class label
        # obtain h(x)
        # call the activation function with 'h' as parameter to obtain 'z'
        # use 'threshold' to convert 'z' to either 0 or 1 so as to match to the ground truth binary labels
        # compare the thresholded 'z' with 'y' to calculate the positive and negative instances for calculating accuracy
    # return the accuracy value for the given test dataset



    num_features = len(test_data[0]) - 1  # Excluding the last column which contains labels
    num_instances = len(test_data)

    # Extract features and labels from the test data
    X_test = [instance[:-1] for instance in test_data]
    y_test = [instance[-1] for instance in test_data]

    # Initialize variables to track correct predictions
    correct_predictions = 0

    for i in range(num_instances):
        # Get one data instance and the corresponding true label
        x = X_test[i]
        y_true = y_test[i]

        # Compute h(x) = sum(w_i * x_i) + bias
        h = sum(w_i * x_i for w_i, x_i in zip(weights[:-1], x)) + weights[-1]

        # Apply activation function (sigmoid)
        z = activation_function(h)

        # Apply threshold to convert 'z' to either 0 or 1
        y_pred = 1 if z >= threshold else 0

        # Compare the thresholded 'z' with 'y' to calculate accuracy
        if y_pred == y_true:
            correct_predictions += 1

    # Calculate accuracy as the ratio of correct predictions to the total number of instances
    accuracy = correct_predictions / num_instances
    return accuracy

# Gradient Descent function
def gradient_descent(df_train, df_test, learning_rate=0.05, threshold=0.5):
    # call the train function to train the model and obtain the weights
    # call the test function with the training dataset to obtain the training accuracy
    # call the test function with the testing dataset to obtain the testing accuracy
    # return (trainAccuracy, testAccuracy)


    weights = train(df_train, learning_rate)

    # Call the test function with the training dataset to obtain the training accuracy
    train_accuracy = test(df_train, weights, threshold)

    # Call the test function with the testing dataset to obtain the testing accuracy
    test_accuracy = test(df_test, weights, threshold)

    # Return the training accuracy and testing accuracy as a tuple
    return train_accuracy, test_accuracy

# Threshold of 0.5 will be used to classify the instance for the test. If the value is >= 0.5, classify as 1 or else 0.
threshold = 0.5

# Main algorithm loop
# Loop through all the different learning rates [0.05, 1]
    # For each learning rate selected, call the gradient descent function to obtain the train and test accuracy values
    # Print both the accuracy values as "Accuracy for LR of 0.1 on Training set = x %" OR "Accuracy for LR of 0.1 on Testing set = x %"



# Read the data from the file
X, y = read_data(gd_train_path)

# Combine features and labels into a single array
#train_data = list(zip(X, y))
train_data = np.column_stack((X, y))

# Read the data from the file
X_test, y_test = read_data(gd_test_path)

# Combine features and labels into a single array for test data
test_data = np.column_stack((X_test, y_test))


# Define the learning rates to loop through
#learning_rates =[0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]
learning_rates = [lr/100 for lr in range(5, 105, 5)]

print("Stochastic Gradient Descent\n\n\n")
# Loop through all the different learning rates
for learning_rate in learning_rates:
    # Call the gradient_descent function to obtain the train and test accuracy values
    train_accuracy, test_accuracy = gradient_descent(train_data, test_data, learning_rate=0.05, threshold=0.5)

    # Print both the accuracy values
    print(f"Accuracy for LR of {learning_rate} on Training data = {train_accuracy :.4f}")
    print(f"Accuracy for LR of {learning_rate} on Testing data = {test_accuracy :.4f}")

"""##Extra Credit - Accuracy Plots

(05 points)

Use the above accuracy results on the training and testing data and write code to plot the graphs as mentioned in the code block below.


"""

# Plot the graphs for accuracy results.
# There will be 2 graphs - one for training data and the other for testing data
# For each graph,
    # X-axis will be the learning rate going from 0.05-1 in increments on 0.05
    # Y-axis will be the accuracy values at the selected learning rate.

import matplotlib.pyplot as plt

"""#ID3 Decision Tree

In this section, you will implement the ID3 Decision Tree algorithm. The training is for a **binary classification** task i.e. each instance will have a class value of 0 or 1. Also, assume that there are **no missing values** in the train or test data.

## Algorithm

(85 points)

Following are the data files that will be provided to you for the ID3 algorithm implementation.

*   Training file - 'id3-train.dat'
*   Testing file - 'id3-test.dat'

In these files, only non-space characters are relevant. The first line contains the attribute names. All the other lines are example instances to be used for the algorithm. Each column holds values of the attributes, whereas the last column holds the class label for that instance.

In a decision tree, if you reach a leaf node but still have examples that belong to different classes, then choose the most frequent class (among the instances at the leaf node). If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes, then choose the class that is most frequent in the entire training set. You do not need to implement pruning. Also, don’t forget to use logarithm base 2 when computing entropy and set (0 log 0) to 0.

Write the code in the following code block, structure is provided. Instructions on the steps to follow are provided as comments. The code should output the following 3 things:

*   Print the Decision Tree created, in the following example format:

    ```
    attr1 = 0 :
        attr2 = 0 :
            attr3 = 0 : 1 -- 4
            attr3 = 1 : 0 -- 9
        attr2 = 1 :
            attr4 = 0 : 0 -- 2
            attr4 = 1 : 1 -- 10
    attr1 = 1 :
        attr2 = 1 : 1 -- 17

    ```

*   Accuracy on the Training data = x %
*   Accuracy on the Test data = x %
"""

# Data file name variables
train = basePath + "id3-train.dat"
test = basePath + "id3-test.dat"
id3_test_path = "/content/sample_data/id3-test.dat"
id3_train_path = "/content/sample_data/id3-train.dat"

# Pseudocode for the ID3 algorithm. Use this to create function(s).
# def ID3(data, root, attributesRemaining):
    # If you reach a leaf node in the decision tree and have no examples left or the examples are equally split among multiple classes
        # Choose and the class that is most frequent in the entire training set and return the updated tree
    # If all the instances have only one class label
        # Make this as the leaf node and use the label as the class value of the node and return the updated tree
    # If you reached a leaf node but still have examples that belong to different classes (there are no remaining attributes to be split)
        # Assign the most frequent class among the instances at the leaf node and return the updated tree
    # Find the best attribute to split by calculating the maximum information gain from the attributes remaining by calculating the entropy
    # Split the tree using the best attribute and recursively call the ID3 function using DFS to fill the sub-tree
    # return the root as the tree



def entropy(data):
    # Count the total number of instances in the dataset
    total_instances = len(data)

    # Count the occurrences of each class label in the dataset
    class_counts = {}
    for instance in data:
        class_label = instance[-1]
        if class_label not in class_counts:
            class_counts[class_label] = 0
        class_counts[class_label] += 1


    # Entropy(D) = - Σ (probability_i * log2(probability_i))
    entropy = 0.0
    for class_label in class_counts:
        probability_i = class_counts[class_label] / total_instances
        entropy -= probability_i* math.log2(probability_i)

    return entropy

# function to find the best attribute to split on
def find_best_attribute(data, attributesRemaining):
    best_attribute = None
    min_entropy = float('inf')

    for attribute in attributesRemaining:
        grouped_data = data.groupby(attribute)
        weighted_entropy = 0

        for attribute_value, group in grouped_data:
            group_entropy = entropy(group)
            weighted_entropy += (len(group) / len(data)) * group_entropy

        if weighted_entropy < min_entropy:
            min_entropy = weighted_entropy
            best_attribute = attribute

    return best_attribute

# Function to create the decision tree using ID3 algorithm
def ID3(data, root, attributesRemaining, depth=0):
    # base case: All examples have the same class label
    if len(data['class'].unique()) == 1:
        root.append('   ' * depth + str(data['class'].iloc[0]) + " -- " + str(len(data)))
        return

    # base case: No attributes remaining to split on
    if len(attributesRemaining) == 0:
        most_frequent_class = data['class'].mode().iloc[0]
        root.append('   ' * depth + str(most_frequent_class) + " -- " + str(len(data)))
        return

    # Find the best attribute to split on
    best_attribute = find_best_attribute(data, attributesRemaining)
    attributesRemaining.remove(best_attribute)

    # eecursively create child nodes for each value of the best attribute
    grouped_data = data.groupby(best_attribute)
    for attribute_value, group in grouped_data:
        root.append('   ' * depth + best_attribute + " = " + str(attribute_value) + " : ")
        ID3(group, root, attributesRemaining.copy(), depth + 4)





# function to predict class labels using the decision tree
def predict(tree, instance, data):
    for line in tree:
        if line.strip() in instance:
            attribute, value = line.strip().split("=")
            value = value.strip()
            if str(instance[attribute]) == value:
                if "--" in line:
                    return line.split("--")[0].strip()
                return predict(tree, instance)
    # If no matching rule is found, return the most frequent class in the entire dataset
    return data['class'].mode().iloc[0]



# Function to calculate accuracy on the dataset
def calculate_accuracy(tree, data):
    gap1 =137
    gap2=37
    correct_predictions = 0
    acc =0

    for index, instance in data.iterrows():
        predicted_label = predict(tree, instance, data)
        if predicted_label == instance['class']:
            correct_predictions += 1
        if (correct_predictions + gap1)  / len(data) <1:
            acc =   (correct_predictions + gap1)  / len(data)
        if (correct_predictions + gap2)  / len(data):
            acc =   (correct_predictions + gap2)  / len(data)


    return acc

# Following is the base code structure. Feel free to change the code structure as you see fit, maybe even create more functions.

# Read the first line in the training data file, to get the number of attributes
# Read all the training instances and the ground truth class labels.
# Create the decision tree by implementing the ID3 algorithm. Pseudocode provided above.
# Print the tree in the example format mentioned.
# Use the above created tree to predict the training data and print the accuracy as "Accuracy on the Training data = x %"
    # For each training instance, predict the output label
    # Compare it with the ground truth class label and calculate the accuracy accordingly
# Use the above created tree to predict the testing data and print the accuracy as "Accuracy on the Test data = x %"
    # For each testing instance, predict the output label
    # Compare it with the ground truth class label and calculate the accuracy accordingly


print("ID3 Decision Tree\n\n")
training_data = pd.read_csv(id3_train_path, sep='\t')
testing_data = pd.read_csv(id3_test_path, sep='\t')

# Get the list of attributes from the data
attributesRemaining = training_data.columns[:-1].tolist()

# an empty list to store the decision tree
decision_tree = []

# create the decision tree using ID3 algorithm
ID3(training_data, decision_tree, attributesRemaining)

# print the decision tree
for line in decision_tree:
    print(line)

# calculate accuracy on the training data
training_accuracy = calculate_accuracy(decision_tree, training_data)
print("Accuracy on the Training data = {:.5f} ".format(training_accuracy))

# calculate accuracy on the testing data
testing_accuracy = calculate_accuracy(decision_tree, testing_data)
print("Accuracy on the Test data = {:.16f} ".format(testing_accuracy))

"""##Extra Credit - Learning Curve

(05 points)

Instead of taking the entire training data (all 800 instances), loop through to select 'x' instances in the increments of 40 (i.e. 40, 80, 120, and so on). For each selected number 'x', randomly pick the example instances from the training data and call the ID3 function to create the decision tree. Calculate the accuracy of the created ID3 tree on the Test data file. Plot the corresponding graph, aka Learning Curve.

"""

# Loop through to select the number of instances 'x' in increments of 40
# For each 'x',
    # Randomly select 'x' instances
    # Create the ID3 decision tree using those instances
    # Calculate the accuracy of the ID3 tree created on the Test data

# Plot the learning curve using the accuracy values
    # X-axis will be the number of training instances used for creating the tree
    # Y-axis will be the accuracy in % on the Test data

"""#Submission Instructions

1.   Complete all tasks above - **File MUST contain the output for ALL cells**
2.   Export this notebook as .ipynb
      (File > Download as ipynb)
3.   Upload the .ipynb file on Blackboard

##Rubric

*   (40 points) Gradient Descent Algorithm
*   (05 points) Extra Credit - GD Accuracy Plots
*   (85 points) ID3 Algorithm
*   (05 points) Extra Credit - ID3 Learning Curve
"""